\subsection{Проблемы автоматического определения тональности}

Проблемы, возникающие при автоматическом определении тональности, можно разделить на общие и специфичные для микроблогов.

\vspace{0.5cm}

Среди общих проблем можно выделить следующие:

\begin{enumerate}

\item{
	Зависимость значения тональности от предметной области. Так, в области фильмов слово ``непредсказуемый'' может иметь положительный оттенок, однако в области клиентского обслуживания это не так.
	При использовании методов обучения с учителем, алгоритм классификации (например, наивный Байесовский классификатор) сам формирует значения тональности из обучающей выборки, поэтому для правильной классификации достаточно, чтобы обучающая и тестовые выборки имели общую предметную область.
	Однако, на практике запросы пользователей не обязательно ограничены какой-либо одной областью, поэтому возможно применение двухпроходной категоризации текста: сначала осуществляется тематическая классификация документа, затем классификация тональности.
}

\item{
	Использование отрицания может изменить тональность остальной части высказывания на обратную. Например, в высказывании ``Раньше мне \textit{очень нравились} смартфоны Nokia. Качество сборки в Lumia 800, действительно, \textit{на высоте}. \textit{Однако}, Windows Phone 7 \textit{всё портит}''. В первом и втором предложении автор высказывает положительное мнение, но из-за использование отрицания в третьем предложении общая тональность относительно объекта ``Nokia'' отрицательная. 

	Исследования в данной области находятся на очень ранней стадии~\cite{negation}.
	
	Для методов машинного обучения, использующих модель типа ``набор слов'' (\textit{bag-of-words}), в качестве простой эвристики можно искусственно добавлять частицу ``не'' к соседним словам (к примеру, для высказывания ``Мне не очень нравится камера iPhone'' получится строка ``Мне не не\_очень не\_нравится камера iPhone''), однако это не очень точное моделирование отрицания (более того, оно может быть выражено неявно).	
}

\item{
	Использование сарказма плохо поддаётся автоматическому определению. Высказывания, содержащие сарказм могут иметь общую тональность, обратную тональности отдельных слов (``\textit{Отличная} книга для страдающих бессонницей!''), или выражать мнение в скрытой форме (вопрос ``Где я?'' в обзоре GPS-навигатора). В этом случае, сарказм может с трудом распознаваться даже людьми. В одной из последних работ в данной области авторам удалось добиться точности на уровне 78\% на коллекции отзывов на товары, используя метод частичного обучения (\textit{semi-supervised learning})~\cite{sarcasm}.
}

\item{
	Значение тональности зависит от перспективы того, кому необходимо провести анализ. Так, для компании Samsung высказывание ``У Samsung показывает отличные продажи :)'' несёт положительное значение, однако для компании Nokia это не так.
}

\end{enumerate}

\vspace{0.5cm}

К специфичным для Твиттера проблемам относятся:

\begin{enumerate}

\item{
	Большой словарь употребимых слов.  Как показывают исследования~\cite{sparsity}, 93\% встречающихся слов употребляются менее, чем 10 раз (78\% на корпусе рецензий фильмов из IMDB). Это объясняется обильным использованием сленга, намеренным и ненамеренным искажением написания слов, использование разных регистров при написании одного и того же слова.
}

\item{
	Короткие сообщения. Из-за ограничения на длину сообщения, средняя длина сообщения на английском составляет 14 слов или 78 символов~\cite{distsuperv}, что отличает анализ Твиттера от анализа более длинных текстов (записей в блогах, рецензий на фильмы и товары). 
}

\item{
	Необходимость обрабатывать большой объём сообщений. Пользователи публикуют более 400 миллионов сообщений в день, 1\% сообщений доступен всем желающим, 50\% доступны для компаний-партнёров~\cite{streaming}. Даже 1\% --- это более 40 миллионов сообщений в день, что является достаточно серьёзным объёмом для обработки локально. Так как все сообщения можно сначала обработать независимо, а затем агрегировать результаты, разумным может быть применение распределённых вычислений, например, используя парадигму MapReduce~\cite{largescaleml, sentiment_mapreduce}.
}

\end{enumerate}