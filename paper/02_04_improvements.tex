\subsection{Улучшение точности работы алгоритмов}

Существует несколько способов улучшить качество работы алгоритмов машинного обучения.

\subsubsection{Выбор признаков}
Выбор признаков (\textit{feature selection}) в первую очередь используется для 
уменьшения размерности признакового пространства. Но так как в данных
присутствует ``шум'' (очень редкие или наоборот очень частые слова, которые
встречаются во всех классах), то уменьшение размерности может улучшить качество и скорость работы алгоритмов. 

Существуют следующие способы Выбор признаков:
\begin{enumerate}

\item
На основе частоты встречаемости слова. Самый простой метод: исключаем
из признакового пространства все слова, которые встречались в обучающей
выборке менее $m$ раз. Число $m$ подбирается экспериментально.

\item
На основе взаимной информации (\textit{mutual information})~\cite{mutualinformation}.
Значение взаимной информации показывает взаимосвязь признака
и класса. Для каждого класса берётся $m$ признаков с наибольшими значениями.

Для того, чтобы посчитать значение взаимной информации необходимо рассчитать
следующие значения для каждого слова $word$ и для каждого класса $c$:
\begin{itemize}

\item $N_{1,1}$ --- сколько раз $word$ встречается в документах класса $c$.

\item $N_{0,1}$ --- сколько раз другие слова встречаются в документах класса $c$.

\item $N_{1,0}$ --- сколько раз $word$ встречается в документах других классов.

\item $N_{0,0}$ --- сколько раз другие слова встречаются в документах других классов.

\item $N = N_{1,1} + N_{0,1} + N_{1,0} + N_{0,0}$

\end{itemize}

Тогда значение взаимной информации считается по формуле:

\begin{equation}
M_{word, c} = 
\frac{N_{1,1}}{N} \log \frac{N N_{1,1}} {N_{1,.} N_{.,1}} +
\frac{N_{0,1}}{N} \log \frac{N N_{0,1}} {N_{0,.} N_{.,1}} +
\frac{N_{1,0}}{N} \log \frac{N N_{1,0}} {N_{1,.} N_{.,0}} +
\frac{N_{0,0}}{N} \log \frac{N N_{0,0}} {N_{0,.} N_{.,0}}
\end{equation}

Индекс с точкой означает, что берётся сумма по значениям 0 и 1 вместо точки 
($N_{1, .} = N_{1, 1} + N_{1, 0}$).

\item
На основе метрики Delta-Idf. Значение Delta-Idf следующим образом:
\begin{equation}
V_{t, d} = \log \frac{|P|}{P_t} - \log \frac{|N|}{N_t}
\end{equation}

где 
\begin{itemize}

\item
$P$ --- множество документов из обучающей выборки, класс которых положительный.

\item
$P_t$ --- количество документов из $P$, в которых встречается терм $t$.

\item
$N$ --- множество документов из обучающей выборки, класс которых отрицательный.

\item
$N_t$ --- количество документов из $N$, в которых встречается терм $t$.

\end{itemize}

\end{enumerate}

В целом все методы показывают примерно одинаковые результаты. 
На корпусе SemEval при использовании наивного
Байесовского классификатора с выбором 1000 признаков
с максимальным значением Delta-Idf значение $F_1$ составило 0.683, 
что лишь немногим хуже результата словарного метода. 

\subsubsection{Использование ансамблей моделей}
Идея использования ансамблей классификаторов состоит в том,
что несколько более простых моделей могут давать результаты
лучше, чем одна сложная модель.

Один из популярных способов (Bagging) заключается в том, что
на частях обучающей выборки тренируется множество моделей,
затем при классификации их результаты усредняются или 
берётся самый популярный результат.
При усреднении для классификаторов могут использоваться разные веса.

\subsubsection{Использование дополнительных признаков}
В качестве классификационных признаков можно так же использовать:
\begin{itemize}

\item
Информацию о частях речи (так называемые, part-of-speech-теги (POS)-теги).

\item
Другие языковые модели (не n-граммы).

\item
Извлекать информацию из хештэгов.

\end{itemize}