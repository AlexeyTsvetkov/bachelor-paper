\subsection{Алгоритмы классификации}

\subsubsection{Наивный Байесовский классификатор}
Наивный Байесовский классификатор основан на теореме Байеса~\cite{naive_bayes}:
\begin{equation}
P(c | d) = \frac{P(d | c) P(c)}{P(d)}
\end{equation}

где
\begin{itemize}

\item
$P(c | d)$ --- апостериорная вероятность того, что документ $d$ принадлежит классу $c$.

\item
$P(d | c)$ --- правдоподобие, вероятность встретить документ $d$ среди всех документов класса $c$.

\item
$P(c)$ --- априорная вероятность класса $c$. 

\item
$P(d)$ --- априорная вероятность документа $d$. 

\end{itemize}

В качестве результирующего выбирается класс с максимальной апостериорной вероятностью:
\begin{equation}
c = \arg\!\underset{c \in \mathbb{C}}{\max} P(c | d)
\end{equation}

Так как плотности распределения чаще всего неизвестны, производится их оценка
по обучающей выборке. При этом оценка вероятности документа в обучающей 
выборке $P(d) = const$, а следовательно не влияет на классификацию.
\begin{equation}
c = \arg\!\underset{c \in \mathbb{C}}{\max} P(d | c) P(c)
\end{equation}

Наивный Байесовский классификатор основан на дополнительном предположении
о том, что классифицирующие признаки являются независимыми, поэтому
условная вероятность документа $P(d | c) \approx P(w_1 | c) P(w_2 | c) \ldots P(w_{|\mathbb{V}|} | c) $, где $w_i \in d$.
\begin{equation}
P(c | d) \approx P(c) \prod\nolimits_{i=1}^{|\mathbb{V}|} P(w_i | c) 
\end{equation}

Для того, чтобы избежать переполнения снизу, обычно произведение
логарифмируется (это изменяет численные значения вероятностных оценок, 
однако значение, при котором достигается максимум, сохраняется).

\begin{equation}
P(c | d) \propto \log P(c) + \sum\nolimits_{i=1}^{|\mathbb{V}|} \log P(w_i | c) 
\end{equation}

Вероятностные оценки определяются следующим образом:

\begin{equation}
P(c) = \frac{\mathbb{D}_c}{|\mathbb{D}|}
\end{equation}

\begin{equation}
P(w_i | c) = \frac{W_{ic}}{\sum\nolimits_{j=1}^{|\mathbb{V}|} W_{jc}}
\end{equation}

где 
\begin{itemize}

\item
$W_{ic}$ --- количество раз сколько i-й терм встречается в документах класса c.

\item
$\mathbb{D}_c$ --- количество документов $d \in \mathbb{D}$ класса c.

\end{itemize}

Достоинства наивного Байесовского классификатора:
\begin{itemize}

\item
Простота реализации.

\item
Быстрый процесс обучения. Вычислительная трудоёмкость обучения $O(|\mathbb{D}| |\mathbb{V}|)$.

\item
Несмотря на то, что предположение о независимости классификационных 
признаков не является верным в естественном языке (значение слова зависят от контекста), 
НБК часто показывает хорошие результаты при текстовой классификации~\cite{Pang2002}. 

\end{itemize}

Недостатки:
\begin{itemize}

\item
Значения, возвращаемые при классификации, нельзя трактовать, 
как вероятности. Таким образом нельзя ответить на вопрос, 
с какой долей уверенности получился результирующий класс. 

\item
Так как в естественном языке слова не являются независимыми,
НБК не является оптимальным.

\end{itemize}

\subsubsection{Классификация методом максимальной энтропии}

Для каждого признака $w_i$ и каждого класса $c_i \in \mathbb{C}$
определим функцию

\begin{equation}
F_{i, c_i}(d, c) = \left\{
\begin{array}{c l}
    1 & {w_i > 0 \wedge c = c_i}  \\   
    0 & {w_i \leq 0\vee c \neq  c_i}
\end{array}\right.
\end{equation}

Оценка вероятности $P(c | d)$ определяется следующим образом:
\begin{equation}
P(c | d, \lambda) = \frac{1}{Z(d)} \exp{\sum\limits_{i'} \lambda_{i'} F_{i', c}}
\end{equation}

где $Z(d)$ --- функция нормировки:

\begin{equation}
Z(d) = \sum\limits_{c' \in \mathbb{C}} \exp{\sum\limits_{i'} \lambda_{i'} F_{i', c'}}
\end{equation}

$\lambda_{i'}$ --- вес i-го признака. Так как, в отличии от наивного Байеса,
метод максимальной энтропии не делает никаких предположений
об условной независимости признаков, то $\lambda_{i'}$ необходимо
восстанавливать из обучающей выборки.

Это можно сделать, максимизировав функцию логарифмического правдоподобия:
\begin{equation}
L(\mathbb{D}, \lambda) = \sum\limits_{\tuple{d, c} \in \mathbb{D}} \log P(c|d, \lambda)
\end{equation}

Так как все значения $P(c|d, \lambda)$ нормированы по единице, а логарифм является выпуклой функцией, то для максимизации можно воспользоваться
методом градиентного подъёма.

Достоинства метода:
\begin{itemize}

\item
Из-за отсутствия предположений об условной независимости признаков
теоретически может показывать лучшую производительность, чем
НБК.

\end{itemize}

Недостатки метода:
\begin{itemize}

\item
Сложная реализация. Необходимо реализовать градиентный подъём,
определить критерий сходимости, параметр шага.

\item
Обучение медленнее, чем в случае НБК.

\end{itemize}